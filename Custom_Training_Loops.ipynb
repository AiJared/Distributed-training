{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGYCq1hSGl/RCK/06/g3gn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kerriea-star/Distributed-training/blob/main/Custom_Training_Loops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial demonstrates how to use `tf.distribute.Strategy`—a TensorFlow API that provides an abstraction for **distributing your training** across multiple processing units (GPUs, multiple machines, or TPUs)—with custom training loops. In this example, you will train a simple convolutional neural network on the **Fashion MNIST** dataset containing 70,000 images of size 28 x 28.\n",
        "\n",
        "**Custom training loops** provide flexibility and a greater control on training. They also make it easier to debug the model and the training loop."
      ],
      "metadata": {
        "id": "etSpnQm001iR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUftS19Of3Nw",
        "outputId": "b89f3fce-74b4-4657-eefe-6f4d65dfb1d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13.0\n"
          ]
        }
      ],
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download the Fashion MNIST dataset"
      ],
      "metadata": {
        "id": "JLKfc_BY12mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Add a dimension to the array -> new shape == (28, 28, 1)\n",
        "# This is done because the first layer in our model is a convolutional\n",
        "# layer and it requires a 4D input (batch_size, height, width, channels).\n",
        "# batch_size dimension will be added later on.\n",
        "\n",
        "train_images = train_images[..., None]\n",
        "test_images = test_images[..., None]\n",
        "\n",
        "# Scale the images to the [0, 1] range.\n",
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEVEBCkY1UXm",
        "outputId": "1b7334cf-7ce7-4cdc-a7ea-158f395baf2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a strategy to distribute the variables and the graph\n",
        "\n",
        "How does `tf.distribute.MirroredStrategy` strategy work?\n",
        "\n",
        "*   All the variables and the model graph are replicated across the replicas.\n",
        "\n",
        "*   Input is evenly distributed across the replicas.\n",
        "\n",
        "*   Each replica calculates the loss and gradients for the input it received.\n",
        "\n",
        "*   The gradients are synced across all the replicas by **summing** them.\n",
        "\n",
        "*   After the sync, the same update is made to the copies of the variables on each replica.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKWMM2kv3W4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the list of devices is not specified in\n",
        "# `tf.distribute.MirroredStrategy` constructor, they will be auto-detected.\n",
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "metadata": {
        "id": "CdHG_lF622fQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWChEHWT4O2W",
        "outputId": "7eb13b7a-df30-4241-a946-6db7214ec668"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup input pipeline"
      ],
      "metadata": {
        "id": "ys9i7QoK49DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(train_images)\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "SWgonJo34dqj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the datasets and distribute them:"
      ],
      "metadata": {
        "id": "0SSHqVzP5W-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "metadata": {
        "id": "uLT7g0fW5Umt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create the model\n",
        "Create a model using `tf.keras.Sequential`. You can also use the **Model Subclassing API** or the **functional API** to do this."
      ],
      "metadata": {
        "id": "8OzNKAekmmLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  regularizer = tf.keras.regularizers.L2(1e-5)\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3,\n",
        "                             activation='relu',\n",
        "                             kernel_regularizer=regularizer),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Conv2D(64, 3,\n",
        "                             activation='relu',\n",
        "                             kernel_regularizer=regularizer),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64,\n",
        "                            activation='relu',\n",
        "                            kernel_regularizer=regularizer),\n",
        "      tf.keras.layers.Dense(10, kernel_regularizer=regularizer)\n",
        "  ])\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "y7zbLfmF6Lv4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a checkpoint directory to store the checkpoints.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')"
      ],
      "metadata": {
        "id": "Ra5UcrFXnzAN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define the loss function\n",
        "Recall that the loss function consists of one or two parts:\n",
        "\n",
        "*   The prediction loss measures how far off the model's predictions are from the training labels for a batch of training examples. It is computed for each labeled example and then reduced across the batch by computing the average value.\n",
        "*   Optionally, regularization loss terms can be added to the prediction loss, to steer the model away from overfitting the training data. A common choice is L2 regularization, which adds a small fixed multiple of the sum of squares of all model weights, independent of the number of examples. The model above uses L2 regularization to demonstrate its handling in the training loop below.\n",
        "\n",
        "For training on a single machine with a single GPU/CPU, this works as follows:\n",
        "\n",
        "*   The prediction loss is computed for each example in the batch, summed across the batch, and then divided by the batch size.\n",
        "*   The regularization loss is added to the prediction loss.\n",
        "*   The gradient of the total loss is computed w.r.t. each model weight, and the optimizer updates each model weight from the corresponding gradient.\n",
        "\n",
        "With `tf.distribute.Strategy`, the input batch is split between replicas. For example, let's say you have 4 GPUs, each with one replica of the model. One batch of 256 input examples is distributed evenly across the 4 replicas, so each replica gets a batch of size 64: We have `256 = 4*64`, or generally `GLOBAL_BATCH_SIZE = num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`.\n",
        "\n",
        "Each replica computes the loss from the training examples it gets and computes the gradients of the loss w.r.t. each model weight. The optimizer takes care that these gradients are summed up across replicas before using them to update the copies of the model weights on each replica.\n",
        "\n",
        "So, how should the loss be calculated when using a `tf.distribute.Strategy`?\n",
        "\n",
        "*   Each replica computes the prediction loss for all examples distributed to it, sums up the results and divides them by `num_replicas_in_sync * BATCH_SIZE_PER_REPLICA`, or equivently, `GLOBAL_BATCH_SIZE`.\n",
        "*   Each replica compues the regularization loss(es) and divides them by `num_replicas_in_sync`.\n",
        "\n",
        "Compared to non-distributed training, all per-replica loss terms are scaled down by a factor of 1/num_replicas_in_sync. On the other hand, all loss terms -- or rather, their gradients -- are summed across that number of replicas before the optimizer applies them. In effect, the optimizer on each replica uses the same gradients as if a non-distributed computation with GLOBAL_BATCH_SIZE had happened. This is consistent with the distributed and undistributed behavior of Keras Model.fit.\n",
        "\n",
        "How to do this in TensorFlow?\n",
        "\n",
        "*   Loss reduction and scaling is done automatically in Keras `Model.compile` and `Model.fit`\n",
        "*   If you're writing a custom training loop, as in this project, you should sum the per-example losses and divide the sum by the global batch size using `tf.nn.compute_average_loss`, which takes the per-example losses and optional sample weights as arguments and returns the scaled loss.\n",
        "*   If using `tf.keras.losses` classes (as in the example below), the loss reduction needs to be explicitly specified to be one of `NONE` or `SUM`. The default `AUTO` and `SUM_OVER_BATCH_SIZE` are disallowed outside Model.fit.\n",
        "\n",
        "  *   `AUTO` is disallowed because the user should explicitly think about what reduction they want to make sure it is correct in the distributed case.\n",
        "  *   `SUM_OVER_BATCH_SIZE`is disallowed because currently it would only divide by per replica batch size, and leave the dividing by number of replicas to the user, which might be easy to miss. So, instead, you need to do the reduction yourself explicitly.\n",
        "\n",
        "\n",
        "*   If you're writing a custom training loop for a model with a non-empty list of Model.losses (e.g., weight regularizers), you should sum them up and divide the sum by the number of replicas. You can do this by using the tf.nn.scale_regularization_loss function. The model code itself remains unaware of the number of replicas.\n",
        "\n",
        "However, models can define input-dependent regularization losses with Keras APIs such as `Layer.add_loss(...)` and `Layer(activity_regularizer=...)`. For `Layer.add_loss(...)`, it falls on the modeling code to perform the division of the summed per-example terms by the per-replica(!) batch size, e.g., by using `tf.math.reduce_mean()`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AL_kSh9Tp_hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  # Set reduction to `NONE` so you can do the reduction yourself\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True,\n",
        "      reduction=tf.keras.losses.Reduction.NONE\n",
        "  )\n",
        "\n",
        "  def compute_loss(labels, predictions, model_losses):\n",
        "    per_example_loss = loss_object(labels, predictions)\n",
        "    loss = tf.nn.compute_average_loss(per_example_loss)\n",
        "    if model_losses:\n",
        "      loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "beqqMfGQoFCL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-DH8ZcIu5-0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}