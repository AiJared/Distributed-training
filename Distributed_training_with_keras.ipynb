{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwRoh4L/BNTuSVihhO8tMD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kerriea-star/Distributed-training/blob/main/Distributed_training_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "The `tf.distribute.Strategy` API provides an abstraction for distributing your training across multiple processing units. It allows you to carry out distributed training using existing models and training code with minimal changes.\n",
        "\n",
        "This project demonstrates how to use the `tf.distribute.MirroredStrategy` to perform in-graph replication with synchronous training on many GPUs on one machine. The strategy essentially copies all of the model's variables to each processor. Then, it uses **all-reduce** to combine the gradients from all processors, and applies the combined value to all copies of the model.\n",
        "\n",
        "You will use the `tf.keras` APIs to build the model and `Model.fit` for training it.\n",
        "\n",
        "`MirroredStrategy` trains your model on multiple GPUs on a single machine. For synchronous training on many GPUs on multiple workers, use the `tf.distribute.MultiWorkerMirroredStrategy` with the Keras Model.fit or a custom training loop."
      ],
      "metadata": {
        "id": "FdjioDWiFoOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "L1pF1BcuIDwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lR4BalDldrMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf35000-9c15-442e-a009-d08acb30b9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhlQ9s-2ISIr",
        "outputId": "73b9c962-88fe-4241-fe9c-8b1d36a1648b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset\n",
        "Load the MNIST dataset from **TensorFlow Datasets**. This returns a dataset in the `tf.data` format.\n",
        "\n",
        "Setting the `with_info` argument to `True` includes the metadata for the entire dataset, which is being saved here to `info`. Among other things, this metadata object includes the number of train and test examples."
      ],
      "metadata": {
        "id": "GE4VwqgqKYJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets, info  = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
        "\n",
        "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
      ],
      "metadata": {
        "id": "F5EebRiTIZel"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the distribution strategy\n",
        "Create a `MirroredStrategy` object. This will handle distribution and provide a context manager (`MirroredStrategy.scope`) to build your model inside."
      ],
      "metadata": {
        "id": "RBZwUculLYIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "metadata": {
        "id": "0hEHawHjK778"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy5TPpvjLloK",
        "outputId": "3210d05c-773a-4a95-8f68-fb43011abdec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set up the input pipeline\n",
        "When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory and tune the learning rate accordingly."
      ],
      "metadata": {
        "id": "M757DboXMpmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also do info.splits.total_num_examples to get the total\n",
        "# number of examples in the dataset.\n",
        "\n",
        "num_train_examples = info.splits['train'].num_examples\n",
        "num_test_examples = info.splits['test'].num_examples\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "id": "f9XVaAXyL2b0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function that normalizes the image pixel values from the `[0, 255] `range to the `[0, 1]` range (**feature scaling**):"
      ],
      "metadata": {
        "id": "Mh4OFdPyNbpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "5U8ZK-_RNSae"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply this scale function to the training and test data, and then use the `tf.data.Dataset` APIs to shuffle the training data (`Dataset.shuffle`), and batch it (`Dataset.batch`). Notice that you are also keeping an in-memory cache of the training data to improve performance (`Dataset.cache`)."
      ],
      "metadata": {
        "id": "-cRa6nKiOBKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "viaYaqOhNxmP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create the model and instantiate the optimizer\n",
        "Within the context of `Strategy.scope`, create and compile the model using the Keras API:"
      ],
      "metadata": {
        "id": "5_23CflgOo19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "M-CIOdftOhUJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this toy example with the MNIST dataset, you will be using the Adam optimizer's default learning rate of 0.001.\n",
        "\n",
        "For larger datasets, the key benefit of distributed training is to learn more in each training step, because each step processes more training data in parallel, which allows for a larger learning rate (within the limits of the model and dataset)."
      ],
      "metadata": {
        "id": "nqv784REdEI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define the callbacks\n",
        "Define the following Keras Callbacks:\n",
        "\n",
        "*   `tf.keras.callbacks.TensorBoard`: writes a log for TensorBoard, which allows you to visualize the graphs.\n",
        "*   `tf.keras.callbacks.ModelCheckpoint`: saves the model at a certain frequency, such as after every epoch.\n",
        "*   `tf.keras.callbacks.BackupAndRestore`: provides the fault tolerance functionality by backing up the model and current epoch number.\n",
        "*   `tf.keras.callbacks.LearningRateScheduler`: schedules the learning rate to change after, for example, every epoch/batch.\n",
        "\n",
        "\n",
        "For illustrative purposes, add a custom callback called `PrintLR` to display the learning rate in the notebook.\n",
        "\n",
        "**Note**: *Use the `BackupAndRestore` callback instead of `ModelCheckpoint` as the main mechanism to restore the training state upon a restart from a job failure. Since `BackupAndRestore` only supports `eager mode`, in `graph mode` consider using `ModelCheckpoint`.*\n"
      ],
      "metadata": {
        "id": "9UfOqj-ZdbRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the checkpoint directory to store the checkpoints.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Define the name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
      ],
      "metadata": {
        "id": "xDEAHNRjPmAl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for decaying the learning rate.\n",
        "# You cna define any decay function you need.\n",
        "def decay(epoch):\n",
        "  if epoch < 3:\n",
        "    return 1e-3\n",
        "  elif epoch >=3 and epoch < 7:\n",
        "    return 1e-4\n",
        "  else:\n",
        "    return 1e-5"
      ],
      "metadata": {
        "id": "E2-K3kBie0pB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a callback for printing the learning rate at the end of each epoch\n",
        "class PrintLR(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    print('\\nlearning rate for epoch {} is {}'.format(      epoch + 1, model.optimizer.lr.numpy()))"
      ],
      "metadata": {
        "id": "mVs7juIifNp1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put all the callbacks together\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                       save_weights_only=True),\n",
        "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
        "    PrintLR()\n",
        "]"
      ],
      "metadata": {
        "id": "OFwG3Dpkf48u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train and evaluate\n",
        "Now, train the model in the usual way by calling Keras` Model.fit` on the model and passing in the dataset created at the beginning of the project. This step is the same whether you are distributing the training or not."
      ],
      "metadata": {
        "id": "hRzb10_Ngt6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 12\n",
        "model.fit(train_dataset, epochs=EPOCHS, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhsvkO6xgYGp",
        "outputId": "93562342-299a-4f59-a953-1f42e665e34f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "938/938 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9423\n",
            "learning rate for epoch 1 is 0.0010000000474974513\n",
            "938/938 [==============================] - 34s 33ms/step - loss: 0.2007 - accuracy: 0.9423 - lr: 0.0010\n",
            "Epoch 2/12\n",
            "938/938 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9804\n",
            "learning rate for epoch 2 is 0.0010000000474974513\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0662 - accuracy: 0.9804 - lr: 0.0010\n",
            "Epoch 3/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0466 - accuracy: 0.9860\n",
            "learning rate for epoch 3 is 0.0010000000474974513\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0466 - accuracy: 0.9860 - lr: 0.0010\n",
            "Epoch 4/12\n",
            "938/938 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9929\n",
            "learning rate for epoch 4 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0256 - accuracy: 0.9929 - lr: 1.0000e-04\n",
            "Epoch 5/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9939\n",
            "learning rate for epoch 5 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0228 - accuracy: 0.9939 - lr: 1.0000e-04\n",
            "Epoch 6/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9945\n",
            "learning rate for epoch 6 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0209 - accuracy: 0.9945 - lr: 1.0000e-04\n",
            "Epoch 7/12\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9950\n",
            "learning rate for epoch 7 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 26s 28ms/step - loss: 0.0195 - accuracy: 0.9950 - lr: 1.0000e-04\n",
            "Epoch 8/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9962\n",
            "learning rate for epoch 8 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0170 - accuracy: 0.9962 - lr: 1.0000e-05\n",
            "Epoch 9/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9963\n",
            "learning rate for epoch 9 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0167 - accuracy: 0.9963 - lr: 1.0000e-05\n",
            "Epoch 10/12\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9962\n",
            "learning rate for epoch 10 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0165 - accuracy: 0.9962 - lr: 1.0000e-05\n",
            "Epoch 11/12\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9964\n",
            "learning rate for epoch 11 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 26s 28ms/step - loss: 0.0163 - accuracy: 0.9964 - lr: 1.0000e-05\n",
            "Epoch 12/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9964\n",
            "learning rate for epoch 12 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0162 - accuracy: 0.9964 - lr: 1.0000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ff60cb7f2e0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m7XhFtNBg7eb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}